---
title: "Practical Machine Learning Course Project - Predicting Weight Lifting Exercises"
author: "Craig Covey"
date: "May 16, 2016"
output: html_document
---

<style type="text/css">
  h1 {
   color: #1F3A93
  }
  h2 { 
   color: #3399ff;		
  }
  h3 { 
   color: #446CB3;		
  }
  body, td {
     font-size: 14px;
  }
  code.r{
    font-size: 14px;
  }
  pre {
    font-size: 12px
  }
  img {
    width: 150px;
    height: 250px;
    display: block;
    margin-left: auto;
    margin-right: auto;
  }
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

#Overview
One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify _how well_ they do it. In this project, I will use machine learning and data from accelerometers on the belt, forearm, arm, and dumbell to predict the manner in which they did the exercise. Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: one was the correct exercise while the other four classes correspond to common mistakes.
More information on the dataset and experiment can be found [here](http://groupware.les.inf.puc-rio.br/har): see the section on the Weight Lifting Exercise Dataset. This report will use the R programming language for all machine learning applications.  
![img](/Users/Craig/Dropbox/R/Coursera/Machine Learning/Predicting_Weight_Lifting_Exercises/on-body-sensing-schema.png) 

#Executive Summary
In this report I experimented with two machine learning algorithms to train my model: _decision trees_ and _random forest_. I started with decision trees because it is straighforward and easy to understand. I evaluated decision trees with two different functions: the `train()` function and the `rpart()` function. Surprisingly, there was a significant difference between the two decision tree functions.
```{r, eval=TRUE, echo=FALSE}
table_1 <- matrix(c(2942, 5885, 0.49992, 5092, 5885, 0.8652506), ncol = 3, byrow = TRUE)
colnames(table_1) <- c("Correct", "Total", "Accuracy")
rownames(table_1) <- c("train()", "rpart()")
table_1 <- as.table(table_1)
kable(table_1)
```

In an effort to improve the model I tried a random forest machine learning algorithm. Random forests are versatile, robust and accurate algorithms capable of performing both regression and classification tasks. The differnce between the `train()` and `randomForest()` functions were miniscule.

```{r, eval=TRUE, echo=FALSE}
table_2 <- matrix(c(5881, 5885, 0.9993203, 5879, 5885, 0.9989805), ncol = 3, byrow = TRUE)
colnames(table_2) <- c("Correct", "Total", "Accuracy")
rownames(table_2) <- c("train()", "rpart()")
table_2 <- as.table(table_2)
kable(table_2)
```

I decided to use the random forest algorithm with the train() function to predict the twenty test cases provided by the class. The final result predicted all twenty exercises classes correctly.

```{r, eval=TRUE, echo=FALSE}
table_3 <- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, "B",  "A",  "B",  "A",  "A",  "E",  "D",  "B",  "A",  "A",  "B",  "C",  "B",  "A",  "E",  "E",  "A",  "B",  "B",  "B"), ncol = 20, byrow = TRUE)
colnames(table_3) <- c("_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_")
rownames(table_3) <- c("Case", "Output")
table_3 <- as.table(table_3)
kable(table_3, caption = "Final Prediction")
```

***

#Data Processing

add commentary .....
```{r, eval=TRUE}
## Load Library Packages
library(ggplot2)
library(dplyr)
library(caret)
library(rattle)
library(rpart)
library(rpart.plot)
library(e1071)
library(randomForest)
library(gbm)
library(RCurl)
library(MASS)
library(knitr)

## Download the data
trainingURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testingURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

trainingPath <- "pml-training.csv"
testingPath <- "pml-testing.csv"

## Download the training data
if (url.exists(trainingURL)) {
    if (!file.exists(trainingPath)) {
        download.file(trainingURL, method = "libcurl", destfile = trainingPath)
    }
}

## Download the testing data
if (url.exists(testingURL)) {
    if (!file.exists(testingPath)) {
        download.file(testingURL, method = "libcurl", destfile = testingPath)
    }
}

## Load the data into R
training <- read.csv(trainingPath, header = TRUE, sep = ",", na.strings = c("", "NA"), fill = TRUE,
                     stringsAsFactors = TRUE)
testing <- read.csv(testingPath, header = TRUE, sep = ",", na.strings = c("", "NA"), fill = TRUE,
                    stringsAsFactors = TRUE)
```

***

#Exploratory Data Analysis

Use glimpse() function from the dplyr package to inspect data. I did not include output for report brevity.
```{r, eval=FALSE}
glimpse(training)
```
```{r}
## Find number of subjects and classes
table(training[, c("user_name", "classe")])
```
> Analysis:  
>

***

#Data Transformation

add commentary ....

```{r}
### Partioning the training set
indexTrain <- createDataPartition(y = training$classe, p = 0.7, list = FALSE)
myTraining <- training[indexTrain, ]
myTesting <- training[-indexTrain, ]

### Training Data Set

## Remove the first column. It is not needed it is just the row number
myTraining <- myTraining[ ,-1]

## Count the number of NAs in each column of the training set
countTrainNAs <- c()
for(i in 1:dim(myTraining)[2]) {
    countTrainNAs <- append(countTrainNAs, sum( is.na( myTraining[ ,i])))
}

## Remove columns from training set that have NAs
for(j in dim(myTraining)[2]:1) {
    if (countTrainNAs[j] > 1) {
        myTraining <- myTraining[ , -j]
    }
}

## Now, do the exact same thing to the myTesting data set
myTesting <- myTesting[, -1]

countTestNAs <- c()
for(i in 1:dim(myTesting)[2]) {
    countTestNAs <- append(countTestNAs, sum( is.na( myTesting[ ,i])))
}

## Remove columns from testing set that have NAs
for(j in dim(myTesting)[2]:1) {
    if (countTestNAs[j] > 1) {
        myTesting <- myTesting[ , -j]
    }
}

## Repeat the same process for the testing set

# Remove first column which is a row number column
testing <- testing[, -1]

countTestNAs <- c()
for(i in 1:dim(testing)[2]) {
    countTestNAs <- append(countTestNAs, sum( is.na( testing[ ,i])))
}

## Remove columns from testing set that have NAs
for(j in dim(testing)[2]:1) {
    if (countTestNAs[j] == 20) {
        testing <- testing[ , -j]
    }
}

dim(myTraining)
dim(myTesting)
dim(testing)
```

add commentary ...

```{r}
common <- intersect(names(myTraining), names(testing))
for (p in common) {
    if (class(myTraining[[p]]) == "factor") {
        levels(testing[[p]]) <- levels(myTraining[[p]])
    }
}
```
> Analysis:  
>

***

#Machine Learning Algorithm: Decision Trees

###Using the `train()` function
```{r}
treesModel <- train(classe ~ ., data = myTraining, method = "rpart")
# print fancy decision tree
rattle::fancyRpartPlot(treesModel$finalModel)
## Prediction
pred.trees.train <- predict(treesModel, myTesting)
## Confusion matrix
options(scipen = 999) # disables printing scientific notation
# Out of sample error
confusionMatrix(pred.trees.train, myTesting$classe)$table
# Accuracy
round(confusionMatrix(pred.trees.train, myTesting$classe)$overall, digits = 5)
```
> Analysis:  
> Ouch 50% accuracy!

###Using the `rpart()` function
```{r}
rpartModel <- rpart(classe ~ ., data = myTraining, method = "class")
# print fancy decision tree
fancyRpartPlot(rpartModel)
## Prediction
pred.trees.rpart <- predict(rpartModel, myTesting, type = "class")
## Confusion Matrix
# Out of sample error
confusionMatrix(pred.trees.rpart, myTesting$classe)$table
# Accuracy
confusionMatrix(pred.trees.rpart, myTesting$classe)$overall
```
> Analysis:  
> A much better accuracy of 86%. Apparently, the `rpart()` function is vastly superior to the `train()` function as it relates to decision trees machine learning algorithms. But lets see if we can do better with random forest.

#Machine Learning Algorithm: Random Forests

###Using the `train()` function
```{r}
rf.model <- train(classe ~ ., data = myTraining, method = "rf",
                  trControl = trainControl(method = "cv", number = 10))
## Prediction
pred.rf.train <- predict(rf.model, myTesting)
## Confusion matrix
# Out of sample error
confusionMatrix(pred.rf.train, myTesting$classe)$table
# Accuracy
confusionMatrix(pred.rf.train, myTesting$classe)$overall
```
> Analysis:  
> 

###Using the `randomForest()` function
```{r}
modelRandFor <- randomForest(classe ~ ., data = myTraining)
## Prediction
pred.rf.randFor <- predict(modelRandFor, myTesting)
## Confustion Matrix
# Out of sample error
confusionMatrix(pred.rf.randFor, myTesting$classe)$table
# Accuracy
confusionMatrix(pred.rf.randFor, myTesting$classe)$overall
```
> Analysis:  
> ... The _accuracy percent change_ from the random forest `train()` function to the `randomForest()` function was `-0.0003398471%`.


#Final Prediction: Testing
```{r}
pred.final <- predict(modelRandFor, newdata = testing, type = "class")

print(pred.final)
```

