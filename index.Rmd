---
title: "Practical Machine Learning Course Project - Predicting Weight Lifting Exercises"
author: "Craig Covey"
date: "May 16, 2016"
output: html_document
---

<style type="text/css">
  h1 {
   color: #1F3A93
  }
  h2 { 
   color: #3399ff;		
  }
  h3 { 
   color: #446CB3;		
  }
  body, td {
     font-size: 14px;
  }
  code.r{
    font-size: 14px;
  }
  pre {
    font-size: 12px
  }
  img {
    width: 150px;
    height: 250px;
    display: block;
    margin-left: auto;
    margin-right: auto;
  }
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

#Overview
One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify _how well_ they do it. In this project, I will use machine learning and data from accelerometers on the belt, forearm, arm, and dumbell to predict the manner in which they did the exercise. Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: one was the correct exercise while the other four classes correspond to common mistakes.
More information on the dataset and experiment can be found [here](http://groupware.les.inf.puc-rio.br/har): see the section on the Weight Lifting Exercise Dataset. This report will use the R programming language for all machine learning applications.  
![img](/Users/Craig/Dropbox/R/Coursera/Machine Learning/Predicting_Weight_Lifting_Exercises/on-body-sensing-schema.png) 

#Executive Summary
In this report I experimented with two machine learning algorithms to train my model: _decision trees_ and _random forest_. I started with decision trees because it is straighforward and easy to understand. I evaluated decision trees with two different functions: the `train()` function and the `rpart()` function. Surprisingly, there was a significant difference between the two decision tree functions.
```{r, eval=TRUE, echo=FALSE}
table_1 <- matrix(c(2942, 5885, 0.49992, 5092, 5885, 0.8652506), ncol = 3, byrow = TRUE)
colnames(table_1) <- c("Correct", "Total", "Accuracy")
rownames(table_1) <- c("train()", "rpart()")
table_1 <- as.table(table_1)
kable(table_1, caption = "Decision Tree")
```

In an effort to improve the model I tried a random forest machine learning algorithm. Random forests are versatile, robust and accurate algorithms capable of performing both regression and classification tasks. The differnce between the `train()` and `randomForest()` functions were miniscule.

```{r, eval=TRUE, echo=FALSE}
table_2 <- matrix(c(5881, 5885, 0.9993203, 5879, 5885, 0.9989805), ncol = 3, byrow = TRUE)
colnames(table_2) <- c("Correct", "Total", "Accuracy")
rownames(table_2) <- c("train()", "rpart()")
table_2 <- as.table(table_2)
kable(table_2, caption = "Random Forests")
```

The two random forest models performed very well but unfortunately the `train()` function model incorrectly misclassified several of the classes on the testing data set. The `randomForest()` function model classified all twenty of the testing classes correctly. Below is the output for the final correct random forest model.

```{r, eval=TRUE, echo=FALSE}
table_3 <- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, "B",  "A",  "B",  "A",  "A",  "E",  "D",  "B",  "A",  "A",  "B",  "C",  "B",  "A",  "E",  "E",  "A",  "B",  "B",  "B"), ncol = 20, byrow = TRUE)
colnames(table_3) <- c("_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_",  "_")
rownames(table_3) <- c("Case", "Output")
table_3 <- as.table(table_3)
kable(table_3, caption = "Final Prediction")
```

***

#Data Processing

Both the training and testing data sets come from http://groupware.les.inf.puc-rio.br/har.  
In this section I:  
1. Load all necessary R packages  
2. Download the two csv files from the internet  
3. Load the files into R  
```{r, eval=TRUE}
## Load Library Packages
library(ggplot2)
library(dplyr)
library(caret)
library(rattle)
library(rpart)
library(rpart.plot)
library(e1071)
library(randomForest)
library(gbm)
library(RCurl)
library(MASS)
library(knitr)

## Download the data
trainingURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testingURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

trainingPath <- "pml-training.csv"
testingPath <- "pml-testing.csv"

## Download the training data
if (url.exists(trainingURL)) {
    if (!file.exists(trainingPath)) {
        download.file(trainingURL, method = "libcurl", destfile = trainingPath)
    }
}

## Download the testing data
if (url.exists(testingURL)) {
    if (!file.exists(testingPath)) {
        download.file(testingURL, method = "libcurl", destfile = testingPath)
    }
}

## Load the data into R
training <- read.csv(trainingPath, header = TRUE, sep = ",", na.strings = c("", "NA"), fill = TRUE,
                     stringsAsFactors = TRUE)
testing <- read.csv(testingPath, header = TRUE, sep = ",", na.strings = c("", "NA"), fill = TRUE,
                    stringsAsFactors = TRUE)
```

Note: When loading the csv files into R note the `na.strings = c("", "NA")` parameter. This is important because the data has a lot of blank values. Converting all blank (`""`) values into `NA`s allows me to use the `is.na()` function to remove them in the **Data Transformation** section.

***

#Exploratory Data Analysis

Use `glimpse()` function from the `dplyr` package to inspect data. I did not include the output for report brevity.
```{r, eval=FALSE}
glimpse(training)
```
```{r}
## Find number of subjects and classes
table(training[, c("user_name", "classe")])
```
> Analysis:  
> After careful inspection of the data I observe six participants (`adelmo, carlitos, charles, eurico, jeremy, pedro`) and five different exercise classes (`A, B, C, D, E`). The `classe` variable corresponds to the type of excercise performed. I will use this variable to predict on. There are many columns with values either entirely or the majority `NA`. These columns will need to be removed because they do not contribute to my machine leanring models.

***

#Data Transformation

I partition the training data set using random subsampling into a `myTraining` and `myTesting` data set with 70% going to `myTraining` and the other 30% going to `myTesting`. I will train my machine learning models with the `myTraining` set and evaluate/test them on the `myTesting`. The `testing` data set will be used for the final prediction. 
  
Next, I find all columns in the `myTraining` and `myTesting` data sets that are majority `NA` and remove those columns. They will not add anything to the models. I perform the exact same mechanics to the provided `testing` data set.

```{r}
### Partioning the training set
indexTrain <- createDataPartition(y = training$classe, p = 0.7, list = FALSE)
myTraining <- training[indexTrain, ]
myTesting <- training[-indexTrain, ]

### Training Data Set

## Remove the first column. It is not needed it is just the row number
myTraining <- myTraining[ ,-1]

## Count the number of NAs in each column of the training set
countTrainNAs <- c()
for(i in 1:dim(myTraining)[2]) {
    countTrainNAs <- append(countTrainNAs, sum( is.na( myTraining[ ,i])))
}

## Remove columns from training set that have NAs
for(j in dim(myTraining)[2]:1) {
    if (countTrainNAs[j] > 1) {
        myTraining <- myTraining[ , -j]
    }
}

## Now, do the exact same thing to the myTesting data set
myTesting <- myTesting[, -1]

countTestNAs <- c()
for(i in 1:dim(myTesting)[2]) {
    countTestNAs <- append(countTestNAs, sum( is.na( myTesting[ ,i])))
}

## Remove columns from testing set that have NAs
for(j in dim(myTesting)[2]:1) {
    if (countTestNAs[j] > 1) {
        myTesting <- myTesting[ , -j]
    }
}

## Repeat the same process for the testing set

# Remove first column which is a row number column
testing <- testing[, -1]

countTestNAs <- c()
for(i in 1:dim(testing)[2]) {
    countTestNAs <- append(countTestNAs, sum( is.na( testing[ ,i])))
}

## Remove columns from testing set that have NAs
for(j in dim(testing)[2]:1) {
    if (countTestNAs[j] == 20) {
        testing <- testing[ , -j]
    }
}

dim(myTraining)
dim(myTesting)
dim(testing)
```

I ran into an issue when performing prediction of the `testing` data set. To remedy the problem I had to manually coerce the levels of the `testing` data set to that of the `myTraining` data set. Apparently the column names, levels, and type must be exactly the same between two data sets when using the `predict()` function. See below for the code.

```{r}
common <- intersect(names(myTraining), names(testing))
for (p in common) {
    if (class(myTraining[[p]]) == "factor") {
        levels(testing[[p]]) <- levels(myTraining[[p]])
    }
}
```
> Analysis:  
> Now all three data sets are set up equally and ready for machine learing.

***

#Machine Learning Algorithm: Decision Trees

###Using the `train()` function
```{r}
treesModel <- train(classe ~ ., data = myTraining, method = "rpart")
# print fancy decision tree
rattle::fancyRpartPlot(treesModel$finalModel)
## Prediction
pred.trees.train <- predict(treesModel, myTesting)
## Confusion matrix
options(scipen = 999) # disables printing scientific notation
# Out of sample error
confusionMatrix(pred.trees.train, myTesting$classe)$table
# Accuracy
round(confusionMatrix(pred.trees.train, myTesting$classe)$overall, digits = 5)
```
> Analysis:  
> Ouch 50% accuracy!

###Using the `rpart()` function
```{r}
rpartModel <- rpart(classe ~ ., data = myTraining, method = "class")
# print fancy decision tree
fancyRpartPlot(rpartModel)
## Prediction
pred.trees.rpart <- predict(rpartModel, myTesting, type = "class")
## Confusion Matrix
# Out of sample error
confusionMatrix(pred.trees.rpart, myTesting$classe)$table
# Accuracy
confusionMatrix(pred.trees.rpart, myTesting$classe)$overall
```
> Analysis:  
> A much better accuracy of 86%. Apparently, the `rpart()` function is vastly superior to the `train()` function as it relates to decision trees machine learning algorithms. But lets see if we can do better with random forest.

#Machine Learning Algorithm: Random Forests

###Using the `train()` function
```{r}
rf.model <- train(classe ~ ., data = myTraining, method = "rf",
                  trControl = trainControl(method = "cv", number = 10))
## Prediction
pred.rf.train <- predict(rf.model, myTesting)
## Confusion matrix
# Out of sample error
confusionMatrix(pred.rf.train, myTesting$classe)$table
# Accuracy
confusionMatrix(pred.rf.train, myTesting$classe)$overall
```
> Analysis:  
> The random forest `train()` function is vastly superior than the decision tree model. It only misclassified 4 classes and had an accuracy of 99.93%. Let see how the `randomForest()` function performs.

###Using the `randomForest()` function
```{r}
modelRandFor <- randomForest(classe ~ ., data = myTraining)
## Prediction
pred.rf.randFor <- predict(modelRandFor, myTesting)
## Confustion Matrix
# Out of sample error
confusionMatrix(pred.rf.randFor, myTesting$classe)$table
# Accuracy
confusionMatrix(pred.rf.randFor, myTesting$classe)$overall
```
> Analysis:  
> The `randomForest()` performed very well with 6 misclassification and an accuracy of 99.90%. The _accuracy percent change_ from the random forest `train()` function to the `randomForest()` function was `-0.0003398471%`. 

***

#Final Prediction: Testing
The random forest machine model was vastly superior to the decision trees model. And the two random forest models performed essentially the same.  
However, when it came to actually predicting on the `testing` data set the `train()` misclassified several of the classes.
```{r}
pred.final.tr <- predict(rf.model, newdata = testing)
print(pred.final.tr)
```

 **The randomForest() function classified all correctly.**

```{r}
pred.final.rf <- predict(modelRandFor, newdata = testing, type = "class")
print(pred.final.rf)
```

***